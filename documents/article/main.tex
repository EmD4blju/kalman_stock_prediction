\documentclass[12pt]{article}

%~ Packages
\usepackage{amsmath}   % For math
\usepackage{graphicx}  % For images
\usepackage{float}     % For stronger figure placement
\usepackage{hyperref}  % For links
\usepackage[T1]{fontenc}
\usepackage{geometry} % For margins
\usepackage[utf8]{inputenc} % Polish characters
\usepackage[polish]{babel} % Polish language support
\usepackage{times} % Use Times font
\usepackage{indentfirst} % Indent first paragraph after section
\usepackage{titlesec} % Customize section titles

%~ Document settings
\geometry{a4paper, margin=2.5cm}
% \renewcommand{\familydefault}{\sfdefault} % Switched to Times

% Adjust justification to prevent overfull boxes without hyphenation
\tolerance=1000
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%~ Section formatting
\titleformat{\section}{\Large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}


%~ Document
\begin{document}

%~ Main Page
\begin{titlepage}
    \centering
    
    \includegraphics[width=0.4\textwidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/article/assets/PJATK_PL_sygnet.png}

    {\Large Polsko-Japońska Akademia Technik Komputerowych}
    
    \vspace{0.5cm}
    
    {\large Wydział Informatyki}
    
    \vspace{2cm}
    
    {\huge\bfseries Zastosowanie filtrów Kalmana do poprawy predykcji cen giełdowych przy użyciu sieci LSTM}
    
    \vspace{2cm}
    
    {\Large Praca Dyplomowa}
    
    \vspace{2cm}
    
    \begin{flushleft}
    \begin{tabular}{ll}
        \textbf{Autor:} & Mikołaj Warda (s28034) \\
        \textbf{Kierunek studiów:} & Informatyka \\
        \textbf{Specjalizacja:} & Data Science \\
        \textbf{Promotor:} & dr Sinh Hoa Nguyen Thi \\
    \end{tabular}
    \end{flushleft}
    
    \vfill
    
    {\large \today}
    
\end{titlepage}

%~ Table of Contents
\tableofcontents
\clearpage

%~ Abstract
\begin{abstract}
    
\end{abstract}
\clearpage

%~ Introduction
\section{Wstęp}

Prognozowanie cen akcji odgrywa kluczową rolę w finansach, wspierając inwestorów w podejmowaniu świadomych decyzji zarządzania swoim portfolio.
Chaotyczny charakter rynków sprawia jednak, że trafne przewidywanie notowań pozostaje trudnym zadaniem.
Tradycyjna analiza techniczna bywa niewystarczająca - jest wrażliwa na szum, a wnioski często zawierają element subiektywności.

W ostatnich latach dynamiczny rozwój uczenia maszynowego, zwłaszcza sieci neuronowych, 
znacząco zmienił podejście do modelowania danych czasowych (w tym giełdowych).
Architektury takie jak LSTM (Long Short-Term Memory) potrafią uchwycić złożone zależności i długookresowe relacje w danych, 
co czyni je obiecującymi narzędziami do prognozowania cen, jednak ich skuteczność nadal zależy od jakości danych wejściowych.
Głównym problemem, jest wysoka wrażliwość na losowe wahania, które są nieodłącznym elementem danych finansowych. 
Te zniekształcenia wynikają z nieprzewidywalnych zdarzeń rynkowych.
Model, zamiast uczyć się rzeczywistych trendów, modeluje wspomniane zakłócenia, 
obniżając swoją zdolność do generalizacji na nowych danych. 
W rezultacie, predykcje mogą być obarczone znacznym błędem, 
co podważa ich użyteczność w podejmowaniu decyzji inwestycyjnych.

Można temu zapobiegać stosując techniki filtracji na danych wejściowych.
Jednym z klasycznych narzędzi tego typu jest filtr Kalmana,
który może pełnić rolę modułu wygładzania i korekty obserwacji, podnosząc stabilność i dokładność predykcji.
Trenowanie sieci neuronowej na przefiltrowanych danych przy użyciu filtra Kalmana może zredukować wpływ szumu, 
pozwalając na lepsze uchwycenie istotnych wzorców i trendów w danych.

W ramach niniejszej pracy dyplomowej, podjęto się zbadania skuteczności zastosowania filtru Kalmana jako etapu przetwarzania danych dla modeli LSTM w kontekście prognozowania cen akcji spółki Amazon.com Inc (AMZN) .
Wybór tematu pracy wyniknął z chęci zdobycia wiedzy na temat działania architektury LSTM oraz zbadania wpływu filtracji danych na jakość prognoz.
Dodatkową motywacją była możliwość wszechstronnego rozwoju ze względu na złożony charakter problemu, łączącego zagadnienia z dziedziny uczenia maszynowego, analizy szeregów czasowych oraz teorii filtrów.

W dalszej części pracy, w sekcji \textit{"Podstawy teoretyczne"}, przedstawiono wszelkie niezbędne pojęcia związane z tematem pracy i przebiegiem badań ze szczególnym naciskiem na architekturę sieci LSTM oraz filtr Kalmana.
Następnie, w sekcji \textit{"Metodyka badań"}, opisano podejście badawcze, w tym przygotowanie danych, implementację modelu, metryki oceny oraz wykorzystane narzędzia.
Kolejna sekcja \textit{"Wyniki i dyskusja"} prezentuje uzyskane wyniki eksperymentów wraz z ich analizą i interpretacją.
Na zakończenie, w sekcji \textit{"Podsumowanie i wnioski"}, podsumowano przeprowadzone badania, przedstawiono kluczowe wnioski oraz zasugerowano kierunki dalszych badań w tym obszarze.

%~ Fundamentals
\clearpage
\section{Podstawy teoretyczne}
\subsection{Przewidywanie szeregów czasowych na rynkach finansowych}
Dane finansowe, takie jak np. ceny akcji, zawierają obserwacje tworzące szeregi czasowe.
Innymi słowy, są to dane, które reprezentują zmiany wartości w określonych odstępach czasu.
Formalnie szeregiem czasowym określa się zbiór uporządkowanych obserwacji w czasie, gdzie każda obserwacja jest powiązana z określoną chwilą czasową:
\begin{equation}
    X = \{x_1, x_2, x_3, \ldots, x_n\}
\end{equation}
gdzie \( x_i \) reprezentuje wartość obserwacji w czasie \( t_i \), a \( n \) to liczba obserwacji w szeregu czasowym.

Abstrahując od rynków finansowych, przedstawiono kilka przykładów szeregów czasowych z innych dziedzin, celem lepszego zobrazowania tej koncepcji:
\begin{itemize}
    \item \textbf{Energetyka:} Godzinowe zużycie energii elektrycznej
    \item \textbf{Meteorologia:} Codzienny pomiar temperatury
    \item \textbf{Sport:} Wyniki meczów drużyny na przestrzeni sezonu
\end{itemize}

Najczęściej spotykanym formatem danych finansowych są tzw. dane OHLC.
Przedstawione w takiej postaci informacje tworzą szereg czasowy, w którym każda obserwacja składa się z czterech części:
\begin{itemize}
    \item \textbf{Open (O):} Cena otwarcia - cena, po której dany instrument finansowy rozpoczął notowania w danym okresie.
    \item \textbf{High (H):} Cena najwyższa - najwyższa cena osiągnięta przez instrument finansowy w danym okresie.
    \item \textbf{Low (L):} Cena najniższa - najniższa cena osiągnięta przez instrument finansowy w danym okresie.
    \item \textbf{Close (C):} Cena zamknięcia - cena, po której instrument finansowy zakończył notowania w danym okresie.
\end{itemize}
Na potrzeby tej pracy, wykorzystano składnik \textit{Close}, ze względu na jego powszechne wykorzystanie w analizie i prognozie cen akcji.

Rysunek \ref{fig:amzn_stock_prices} przedstawia przykładowy wykres cenowy względem składnika \textit{Close} spółki Amazon.com Inc (AMZN) na przestrzeni kilku lat w odstępach dziennych. 
Dane przedstawione na wykresie są nieregularne i zawierają liczne fluktuacje, co jest charakterystyczne dla danych finansowych.
Ta wysoka zmienność stanowi główne wyzwanie dla modeli predykcyjnych, co motywuje do poszukiwania skutecznych metod filtrujących.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/1_AMZN_stock.png}
    \caption{Wykres cenowy względem składnika \textit{Close} Amazon.com Inc (AMZN) w latach 2022-2025}
    \label{fig:amzn_stock_prices}
\end{figure}

Tradycyjnie analiza danych rynkowych opierała się o wskaźniki techniczne. 
Można je określić mianem narzędzi statystycznych reprezentujących różne aspekty zachowań cen.
W niniejszej pracy wykorzystano trzy wskaźniki techniczne, które opisano w następnych podsekcjach.

\subsubsection{Wskaźnik siły względnej (Relative Strength Index, RSI)}
Wskaźnik siły względnej (RSI) to popularny wskaźnik techniczny używany do oceny siły i prędkości zmian cen aktywów finansowych.
Wyraża się go wzorem:
\begin{equation}
    RSI = 100 - \frac{100}{1 + RS}
\end{equation}
gdzie \( RS \) (Relative Strength) to stosunek średnich wzrostów do średnich spadków cen w określonym czasie. Według badań, optymalny okres do obliczania RSI wynosi 14 dni.

Produktem końcowym RSI jest liczba z zakresu od 0 do 100, która interpretowana jest następująco:
\begin{itemize}
    \item Wartości powyżej 70 sugerują, że akcja jest wykupiona i może nastąpić spadek cen.
    \item Wartości poniżej 30 sugerują, że akcja jest wyprzedana i może nastąpić wzrost cen.
    \item Wartości pomiędzy 30 a 70 wskazują na neutralny stan rynku.
\end{itemize}

\subsubsection{Wstęgi Bollingera (Bollinger Bands)}
Wstęgi Bollingera to narzędzie analizy technicznej, dostarczające informacji o zmienności rynku. 
Składaja się z trzech linii na wykresie cenowym:
\begin{itemize}
    \item \textbf{Środkowa linia:} Prosta średnia krocząca (SMA) obliczona na podstawie cen zamknięcia w określonym czasie. Najczęsciej używanym okresem jest 20 dni.
    \item \textbf{Górna wstęga:} Obliczana jako suma wartości środkowej linii i dwukrotności odchylenia standardowego cen w tym okresie.
    \item \textbf{Dolna wstęga:} Obliczana jako różnica wartości środkowej linii i dwukrotności odchylenia standardowego cen w tym okresie.
\end{itemize}
Na podstawie ww. składowych można zinterpretować dwie nowe miary, które zostały wykorzystane w niniejszej pracy:
\begin{itemize}
    \item \textbf{Bollinger Bandwidth (BBW):} Miara szerokości wstęg Bollingera, obliczana jako stosunek różnicy między górną a dolną wstęgą do środkowej linii:
    \begin{equation}
        BBW = \frac{Upper Band - Lower Band}{Middle Line}
    \end{equation}
    \item \textbf{Bollinger \%B (BB\%):} Miara położenia ceny względem wstęg Bollingera, obliczana jako stosunek różnicy między ceną zamknięcia a dolną wstęgą do różnicy między górną a dolną wstęgą:
    \begin{equation}
        BB\% = \frac{Close - Lower Band}{Upper Band - Lower Band}
    \end{equation}
\end{itemize}

Opisane powyżej wskaźniki techniczne - RSI, BBW oraz BB\% - dostarczają cennych informacji o dynamice rynku. 
W tradycyjnej analizie mogą posłużyć do subiektywnej interpretacji przez analityków.
W nowoczesnych podejściach, opartych na modelach uczenia maszynowego, 
można je wykorzystać do wzbogacenia danych o dodatkowe cechy, co potencjalnie może poprawić jakość prognoz.

\subsection{Sieci neuronowe w prognozowaniu rynków finansowych}

Wraz z rozwojem mocy obliczeniowej i technik uczenia maszynowego, pojawiły się bardziej zaawansowane metody analizy i prognozowania w dziedzinie finansów.
Do najpopularniejszych podejść należą sieci rekurencyjne (RNN), w tym ich zaawansowane warianty, takie jak \textit{LSTM} (Long Short-Term Memory).
Sieci te są zdolne do uchwycenia złożonych wzorców i zależności w szeregach czasowych.

W niniejszej sekcji zostały omówione podstawy działania sieci neuronowych. 
Przedstawiona została budowa pojedynczego neuronu wchodzącego w skład sieci, kluczowe algorytmy uczenia oraz architektura LSTM.

\subsubsection{Podstawy działania sztucznego neuronu}
Ważnym kamieniem milowym w dziedzinie uczenia maszynowego było wprowadzenie matematycznego modelu neuronu przez McCullocha i Pitts'a w 1943 roku.
Współcześnie można mówić o różnych architekturach takiego neuronu, jednak ich podstawowa zasada działania pozostaje podobna.

Neuron otrzymuje na wejściu sygnały \( x_1, x_2, \ldots, x_n \), które są ważone przez odpowiednie wagi \( w_1, w_2, \ldots, w_n \).
Następnie, sumuje te ważone sygnały i dodaje do nich wartość obciążenia (bias) \( b \):
\begin{equation}
    z = \sum_{i=1}^{n} w_i x_i + b
\end{equation}

Otrzymana wartość \( z \), zwana logitem (surowym wyjściem) neuronu, jest następnie przekształcana przez funkcję aktywacji \( f(z) \).
\begin{equation}
    \hat{y} = f(z) = f(\sum_{i=1}^{n} w_i x_i + b)
\end{equation}

Funkcja aktywacji modyfikuje logit \( z \), produkując finalne wyjście neuronu \( \hat{y} \) -- stopień aktywacji.
W zależności od zastosowanej funkcji aktywacji, wyjście może przyjmować różne formy.
Tą różnicę najprościej zilustrować na przykładzie dwóch popularnych funkcji aktywacji:
\begin{itemize}
    \item \textbf{Funkcja skokowa:}
    \begin{equation}
        f(z) = 
        \begin{cases}
            1 & \text{jeśli } z \geq 0 \\
            0 & \text{jeśli } z < 0
        \end{cases}
    \end{equation}
    W tym przypadku, wyjście neuronu jest binarne. 
    Innymi słowy, neuron zostanie albo aktywowany (1), albo nieaktywowany (0), w zależności od tego, czy logit \( z \) przekracza pewien próg (w tym przypadku 0).
    \item \textbf{Funkcja sigmoidalna:}
    \begin{equation}
        f(z) = \frac{1}{1 + e^{-z}}
    \end{equation}
    W tym przypadku, wyjście neuronu jest ciągłe i mieści się w zakresie od 0 do 1.
    Oznacza to, że neuron może przyjmować różne poziomy aktywacji, co pozwala na wyrażenie stopnia pewności co do wyniku.
\end{itemize}
Rysunek \ref{fig:ste_sigmoid_comparison} przedstawia porównanie ww. funkcji aktywacji.
Po lewej stronie znajduje się wykres funkcji skokowej, gdzie wyjście nagle zmienia się z 0 na 1 w punkcie \( z=0 \).
Po prawej stronie znajduje się wykres funkcji sigmoidalnej, gdzie wyjście zmienia się stopniowo od 0 do 1 wraz ze wzrostem wartości \( z \).
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/step_figure.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/sigmoid_figure.png}
    \end{minipage}
    \caption{Porównanie funkcji aktywacji: (po lewej) funkcja skokowa, (po prawej) funkcja sigmoidalna.}
    \label{fig:ste_sigmoid_comparison}
\end{figure}
Istnieje wiele innych funkcji aktywacji (np. ReLU, tanh, softmax), a każda z nich ma swoje unikalne właściwości i zastosowania.
W przypadku funkcji skokowej, neuron podejmuje twardą decyzję, aktywując się w pełni lub wcale.
Przez brak elastyczności, funkcja ta nie pozwala na wyrażenie przekonania co do wyniku.
W przeciwieństwie do niej, funkcja sigmoidalna umożliwia płynną aktywację, pozwalając uwzględnić stopień pewności.

Znając rolę funkcji aktywacji, można lepiej wyjaśnić działanie parametrów uczonych neuronu, czyli wag \( w_i \) oraz obciążenia \( b \).

Wagi określają, jak duży wpływ ma każdy sygnał wejściowy \( x_i \) na logit \( z \).
Poprzez modelowanie wag, w trakcie procesu uczenia, neuron może nauczyć się, które cechy wejściowe są bardziej istotne dla danego zadania.

Rysunek \ref{fig:sigmoid_weights_combined} przedstawia wpływ różnych wartości wagi \( w \) na funkcję sigmoidalną w neuronie z jednym wejściem i bez obciążenia.
Po lewej stronie znajduje się wykres dla małej wagi \( w=0.1 \), gdzie funkcja zmienia się powoli i ma łagodne nachylenie.
Po prawej stronie znajduje się wykres dla dużej wagi \( w=2.0 \), gdzie funkcja zmienia się gwałtownie i ma strome nachylenie.
Można zaobserwować, że wraz ze wzrostem wartości wagi, funkcja sigmoidalna zaczyna przypominać funkcję skokową.
Oznacza to, że neuron staje się pewniejszy swoich decyzji, aktywując się niemal natychmiast po przekroczeniu progu.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/sigmoid_w_min_figure.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/sigmoid_w_max_figure.png}
    \end{minipage}
    \caption{Wpływ wartości wagi (w) na kształt funkcji sigmoidalnej: (po lewej) \(w=0.1\), (po prawej) \(w=2.0\).}
    \label{fig:sigmoid_weights_combined}
\end{figure}

Drugim kluczowym parametrem jest obciążenie, czyli bias \( b \), który przesuwa próg funkcji aktywacji wzdłuż osi poziomej.
W istocie umożliwia to opóźnienie lub przyspieszenie momentu, w którym neuron zostaje aktywowany.

Rysunek \ref{fig:sigmoid_bias_combined} przedstawia wpływ różnych wartości biasu \( b \) na kształt funkcji sigmoidalnej w neuronie z jednym wejściem i wagą \( w=0.1 \).
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/sigmoid_b_min_figure.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/sigmoid_b_max_figure.png}
    \end{minipage}
    \caption{Wpływ wartości bias (b) na kształt funkcji sigmoidalnej: (po lewej) \(b=-2.82\), (po prawej) \(b=4.21\).}
    \label{fig:sigmoid_bias_combined}
\end{figure}

\clearpage
Opisane powyżej elementy -- wagi \( w_i \), obciążenie \( b \) oraz funkcja aktywacji \( f(z) \) - stanowią podstawę działania sztucznego neuronu.

Rysunek \ref{fig:neuron_architecture} podsumowuje budowę sztucznego neuronu, ilustrując jak sygnały wejściowe są przetwarzane przez wagę, sumowane z obciążeniem i przekształcane przez funkcję aktywacji.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/neuron.png}
    \caption[Schemat budowy sztucznego neuronu]{Schemat budowy sztucznego neuronu. Źródło: Andrut, Krzysztof Zajączkowski, \href{https://commons.wikimedia.org/wiki/File:Neuron_McCullocha-Pittsa.svg}{Wikimedia Commons}, licencja \href{https://creativecommons.org/licenses/by-sa/2.5/deed.pl}{CC BY-SA 2.5}.}
    \label{fig:neuron_architecture}
\end{figure}

Niestety pojedyczny neuron jest ograniczony w swoich możliwościach -- może nauczyć się jedynie prostych zależności liniowo-separowalnych.
Inaczej mówiąc, jest w stanie rozróżnić tylko te wzorce, które można oddzielić prostą linią w przestrzeni cech.
Aby radzić sobie z bardziej złożonymi problemami, konieczne jest łączenie wielu neuronów w wielowarstwowe struktury.

\subsubsection{Architektura sieci neuronowej}

W celu modelowania bardziej złożonych zależności, łączy się wiele neuronów w struktury zwane sieciami neuronowymi.
Składają się one z warstw, gdzie każda warstwa zawiera wiele neuronów.
Typowa sieć neuronowa składa się z trzech głównych typów warstw:
\begin{itemize}
    \item \textbf{Warstwa wejściowa:} Odpowiada za przyjmowanie danych wejściowych.
    \item \textbf{Warstwy ukryte:} Przetwarzają dane poprzez zestaw neuronów, ucząc się złożonych wzorców.
    \item \textbf{Warstwa wyjściowa:} Generuje ostateczne prognozy lub klasyfikacje na podstawie przetworzonych danych.
\end{itemize}
Warstwy można łączyć na różne sposoby.
Popularnym podejściem jest pełne łączenie przy wykorzystaniu warstw gęstych (Dense layers). 
Polega ono na łączeniu każdego neuronu z jednej warstwy do każdego neuronu w następnej warstwie.
Przepływ sygnału w takiej sieci odbywa się na zasadzie propagacji w przód (ang. forward propagation) -- od warstwy wejściowej, przez warstwy ukryte, aż do warstwy wyjściowej.
Formalnie ten proces można opisać krokami obliczeniowymi dla każdej warstwy:
\begin{enumerate}
    \item \textbf{Warstwa wejściowa:}
    \begin{equation}
        \mathbf{a}^{(0)} = \mathbf{x}
    \end{equation}
    \item \textbf{Warstwy ukryte:}
    \begin{equation}
        \mathbf{a}^{(l)} = f(\mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}) \quad \text{dla } l = 1, 2, \ldots, L-1
    \end{equation}
    \item \textbf{Warstwa wyjściowa:}
    \begin{equation}
        \mathbf{\hat{y}} = g(\mathbf{W}^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)})
    \end{equation}
    gdzie:
    \begin{itemize}
        \item \( \mathbf{x} \) to wektor danych wejściowych,
        \item \( \mathbf{\hat{y}} \) to wektor danych wyjściowych (prognoz),
        \item \( \mathbf{a}^{(l)} \) to aktywacje (wyjścia) warstwy \( l \),
        \item \( \mathbf{W}^{(l)} \) to macierz wag warstwy \( l \),
        \item \( \mathbf{b}^{(l)} \) to wektor obciążeń warstwy \( l \),
        \item \( f \) to funkcja aktywacji dla warstw ukrytych,
        \item \( g \) to funkcja aktywacji dla warstwy wyjściowej,
        \item \( L \) to liczba warstw w sieci.
    \end{itemize}
\end{enumerate}

Rysunek \ref{fig:nn_architecture} przedstawia schematyczną budowę prostej sieci neuronowej z jedną warstwą wejściową, dwiema warstwami ukrytymi i jedną warstwą wyjściową.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/emdablju/Documents/projects/kalman_stock_prediction/documents/plots/nn.png}
    \caption{Budowa prostej sieci neuronowej z jedną warstwą wejściową, dwiema warstwami ukrytymi i jedną warstwą wyjściową. Opracowanie własne na podstawie \href{https://media.geeksforgeeks.org/wp-content/uploads/20250923121847731542/Neural-Networks-Architecture.webp}{GeeksforGeeks}.}
    \label{fig:nn_architecture}
\end{figure}





%~ Research approach
% \section{Metodyka badań}

% %~ Results
% \section{Wyniki i dyskusja}

% %~ Summary
% \section{Podsumowanie i wnioski}

% %~ Bibliography
% \section{Bibliografia}

% %~ List of figures and tables
% \section{Spis rysunków i tabel}

% %~ Attachments
% \section{Załączniki}


\end{document}
